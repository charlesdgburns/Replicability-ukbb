{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import random\n",
    "from scipy.stats import pearsonr,spearmanr\n",
    "from scipy import stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fdr multiple testing correction\n",
    "from statsmodels.stats import multitest\n",
    "\n",
    "def fdr_correction(P):\n",
    "    size = P.shape\n",
    "    temp_p = P.flatten()\n",
    "    Ps = multitest.multipletests(temp_p,alpha=0.05,method='fdr_bh')\n",
    "    P_corrected = Ps[1].reshape(size)\n",
    "\n",
    "    return P_corrected"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the function of calculating Jaccard index\n",
    "from sklearn.metrics import confusion_matrix,jaccard_score\n",
    "def Jaccard_index(data1,data2,t=None):\n",
    "    s = data1.shape\n",
    "    odata1 = np.zeros((s[0],2))\n",
    "    for i in range(s[0]):\n",
    "        tmp_l = []\n",
    "        for j in range(s[1]):\n",
    "            corr_map1 = data1[i,j,:,:]\n",
    "            corr_map2 = data2[i,j,:,:]\n",
    "            if t == 'fdr':\n",
    "                correct_P1 = fdr_correction(corr_map1[:,1])\n",
    "                correct_P2 = fdr_correction(corr_map2[:,1])\n",
    "                \n",
    "                l1 = np.where(correct_P1 < 0.05)[0].tolist()\n",
    "                l2 = np.where(correct_P2 < 0.05)[0].tolist()\n",
    "            elif t == 'bonferroni':\n",
    "                pt = 0.05/s[2]\n",
    "                l1 = np.where(corr_map1[:,1] < pt)[0].tolist()\n",
    "                l2 = np.where(corr_map2[:,1] < pt)[0].tolist()\n",
    "            else:\n",
    "                l1 = np.where(corr_map1[:,1] < float(t))[0].tolist()\n",
    "                l2 = np.where(corr_map2[:,1] < float(t))[0].tolist()\n",
    "                \n",
    "            seqs = np.zeros((s[2],2))\n",
    "            seqs[l1,0] = 1\n",
    "            seqs[l2,1] = 1\n",
    "            if len(l1) == 0 and len(l2) == 0:\n",
    "                confusion_matrix1 = np.zeros((2,2))\n",
    "                confusion_matrix1[0,0] = s[2]\n",
    "            else:\n",
    "                confusion_matrix1 = confusion_matrix(seqs[:,0],seqs[:,1])\n",
    "                \n",
    "            confusion_matrix2 = np.zeros((2,2))\n",
    "            confusion_matrix2[0,0] = (confusion_matrix1[0,0] + confusion_matrix1[0,1])*(confusion_matrix1[0,0] + confusion_matrix1[1,0])\n",
    "            confusion_matrix2[1,1] = (confusion_matrix1[1,1] + confusion_matrix1[0,1])*(confusion_matrix1[1,1] + confusion_matrix1[1,0])\n",
    "            confusion_matrix2[0,1] = (confusion_matrix1[1,1] + confusion_matrix1[0,1])*(confusion_matrix1[0,0] + confusion_matrix1[0,1])\n",
    "            confusion_matrix2[1,0] = (confusion_matrix1[1,1] + confusion_matrix1[1,0])*(confusion_matrix1[0,0] + confusion_matrix1[1,0])\n",
    "            \n",
    "            n1 = confusion_matrix1[1,1] + confusion_matrix1[1,0] + confusion_matrix1[0,1]\n",
    "            if n1 == 0:\n",
    "                S = 0\n",
    "            else:\n",
    "                S = confusion_matrix1[1,1]/ n1\n",
    "            \n",
    "            n2 = confusion_matrix2[1,1] + confusion_matrix2[1,0] + confusion_matrix2[0,1]\n",
    "            if n2 == 0:\n",
    "                ES = 0\n",
    "            else:\n",
    "                ES = confusion_matrix2[1,1]/n2\n",
    "            \n",
    "            dd = (S - ES)/(1-ES)\n",
    "            tmp_l.append(dd)\n",
    "                \n",
    "#             l = list(set(l1) & set(l2))\n",
    "#             n = len(l)\n",
    "#             m = len(l1) + len(l2) - len(l)\n",
    "#             if m > 0:\n",
    "#                 tmp_l.append(n/m)\n",
    "#             else:\n",
    "#                 tmp_l.append(0)\n",
    "        odata1[i,0] = np.mean(tmp_l)\n",
    "        odata1[i,1] = np.std(tmp_l)\n",
    "    return odata1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # the function of calculating Intraclass Correlation Coefficient\n",
    "import pingouin as pg\n",
    "from scipy import stats\n",
    "\n",
    "def calculate_ICC(data1,data2,t = None):\n",
    "    subsampling_times = data1.shape[0]\n",
    "    random_num = data1.shape[1]\n",
    "    ICC_data = np.zeros((subsampling_times,2)) \n",
    "    t = float(t)\n",
    "    region_num = round(t * data1.shape[2])\n",
    "    for i in range(subsampling_times):\n",
    "        print(i)\n",
    "        temp_icc = []\n",
    "        for j in range(random_num):\n",
    "            \n",
    "            my_targets = []\n",
    "            my_raters = []\n",
    "            my_ratings = []\n",
    "            \n",
    "            tdata1 = data1[i,j,:,:]\n",
    "            tdata2 = data2[i,j,:,:]\n",
    "            zdata1 = stats.zscore(tdata1[:,0])\n",
    "            zdata2 = stats.zscore(tdata2[:,0])\n",
    "            argindexs1 = np.argsort(tdata1[:,1])[:region_num]\n",
    "            tmp1 = zdata1[argindexs1]\n",
    "            tmp2 = zdata2[argindexs1]\n",
    "            \n",
    "\n",
    "            for k in range(region_num):\n",
    "                my_targets.append(k+1)\n",
    "                my_raters.append('A')\n",
    "                my_ratings.append(tmp1[k])\n",
    "  \n",
    "                my_targets.append(k+1)\n",
    "                my_raters.append('B')\n",
    "                my_ratings.append(tmp2[k])\n",
    "        \n",
    "            my_df_data = pd.DataFrame(data=my_targets,columns=['region'])\n",
    "            my_df_data['random_time'] = my_raters\n",
    "            my_df_data['corr'] = my_ratings\n",
    "            # ICC2: A random sample of k raters rate each target. The measure is one of absolute agreement in the ratings.\n",
    "            my_icc = pg.intraclass_corr(data=my_df_data, targets='region', raters='random_time', ratings='corr')['ICC'].values[1]\n",
    "            temp_icc.append(my_icc)\n",
    "    \n",
    "        ICC_data[i,0] = np.mean(temp_icc)\n",
    "        ICC_data[i,1] = np.std(temp_icc)\n",
    "    return ICC_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# conducting the calculation of Jaccard index\n",
    "file_path = '/data/sliu/sampling_ukbb_analysis/new_results/'\n",
    "files = os.listdir(file_path)\n",
    "\n",
    "# t indictaes the significance thresholds: p<0.05, p<0.01, fdr_p<0.05, fdr_bonferroni<0.05\n",
    "\n",
    "t = 'bonferroni'\n",
    "for f in files:\n",
    "    print(f)\n",
    "    CT_file_path1 = os.path.join(file_path,f) + '/random/random_data_CT1.npy'\n",
    "    CT_file_path2 = os.path.join(file_path,f) + '/random/random_data_CT2.npy'\n",
    "    CSA_file_path1 = os.path.join(file_path,f) + '/random/random_data_CSA1.npy'\n",
    "    CSA_file_path2 = os.path.join(file_path,f) + '/random/random_data_CSA2.npy'\n",
    "    FC_file_path1 = os.path.join(file_path,f) + '/random/random_data_FC1.npy'\n",
    "    FC_file_path2 = os.path.join(file_path,f) + '/random/random_data_FC2.npy'\n",
    "    \n",
    "    random_data_CSA1 = np.load(CSA_file_path1)\n",
    "    random_data_CT1 = np.load(CT_file_path1)\n",
    "    random_data_FC1= np.load(FC_file_path1)\n",
    "        \n",
    "    random_data_CSA2 = np.load(CSA_file_path2)\n",
    "    random_data_CT2 = np.load(CT_file_path2)\n",
    "    random_data_FC2 = np.load(FC_file_path2)\n",
    "    \n",
    "    CT_reliability = Jaccard_index(random_data_CT1,random_data_CT2,t=t)\n",
    "    CSA_reliability = Jaccard_index(random_data_CSA1,random_data_CSA2,t=t)\n",
    "    FC_reliability = Jaccard_index(random_data_FC1,random_data_FC2,t=t)\n",
    "    \n",
    "    CSA_file_name = 'new_results/'+f+'/CSA_Jaccard_index_'+t+'.csv'\n",
    "    CT_file_name = 'new_results/'+f+'/CT_Jaccard_index_'+t+'.csv'\n",
    "    FC_file_name = 'new_results/'+f+'/FC_Jaccard_index_'+t+'.csv'\n",
    "\n",
    "    data1 = pd.DataFrame(data=CT_reliability)\n",
    "    data2 = pd.DataFrame(data=CSA_reliability)\n",
    "    data3 = pd.DataFrame(data=FC_reliability)\n",
    "\n",
    "    data1.to_csv(CT_file_name,index=False)\n",
    "    data2.to_csv(CSA_file_name,index=False)\n",
    "    data3.to_csv(FC_file_name,index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = '/data/sliu/sampling_ukbb_analysis/new_results/'\n",
    "files = os.listdir(file_path)\n",
    "\n",
    "# t indicates how many brain measures are included to calculate the ICC: 10%,15%,20%,25%,50%,100%\n",
    "t = '0.2'\n",
    "for f in files:\n",
    "    print(f)\n",
    "    CT_file_path1 = os.path.join(file_path,f) + '/random/random_data_CT1.npy'\n",
    "    CT_file_path2 = os.path.join(file_path,f) + '/random/random_data_CT2.npy'\n",
    "    CSA_file_path1 = os.path.join(file_path,f) + '/random/random_data_CSA1.npy'\n",
    "    CSA_file_path2 = os.path.join(file_path,f) + '/random/random_data_CSA2.npy'\n",
    "    FC_file_path1 = os.path.join(file_path,f) + '/random/random_data_FC1.npy'\n",
    "    FC_file_path2 = os.path.join(file_path,f) + '/random/random_data_FC2.npy'\n",
    "    \n",
    "    random_data_CSA1 = np.load(CSA_file_path1)\n",
    "    random_data_CT1 = np.load(CT_file_path1)\n",
    "    random_data_FC1= np.load(FC_file_path1)\n",
    "        \n",
    "    random_data_CSA2 = np.load(CSA_file_path2)\n",
    "    random_data_CT2 = np.load(CT_file_path2)\n",
    "    random_data_FC2 = np.load(FC_file_path2)\n",
    "    \n",
    "    CT_reliability = calculate_ICC(random_data_CT1,random_data_CT2,t=t)\n",
    "    CSA_reliability = calculate_ICC(random_data_CSA1,random_data_CSA2,t=t)\n",
    "    FC_reliability = calculate_ICC(random_data_FC1,random_data_FC2,t=t)\n",
    "    \n",
    "    CSA_file_name = 'new_results/'+f+'/CSA_ICC_'+t+'.csv'\n",
    "    CT_file_name = 'new_results/'+f+'/CT_ICC_'+t+'.csv'\n",
    "    FC_file_name = 'new_results/'+f+'/FC_ICC_'+t+'.csv'\n",
    "\n",
    "    data1 = pd.DataFrame(data=CT_reliability)\n",
    "    data2 = pd.DataFrame(data=CSA_reliability)\n",
    "    data3 = pd.DataFrame(data=FC_reliability)\n",
    "\n",
    "    data1.to_csv(CT_file_name,index=False)\n",
    "    data2.to_csv(CSA_file_name,index=False)\n",
    "    data3.to_csv(FC_file_name,index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
