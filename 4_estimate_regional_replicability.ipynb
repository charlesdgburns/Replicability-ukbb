{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import random\n",
    "from scipy.stats import pearsonr,spearmanr\n",
    "from scipy import stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fdr multiple testing correction\n",
    "from statsmodels.stats import multitest\n",
    "\n",
    "def fdr_correction(P):\n",
    "    size = P.shape\n",
    "    temp_p = P.flatten()\n",
    "    Ps = multitest.multipletests(temp_p,alpha=0.05,method='fdr_bh')\n",
    "    P_corrected = Ps[1].reshape(size)\n",
    "\n",
    "    return P_corrected"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the function of calculating regional replicability\n",
    "def estimate_reliability(data1,data2,t=None):\n",
    "    s = data1.shape\n",
    "    odata1 = np.zeros((s[0],s[1],s[2]))\n",
    "    if t == 'fdr':\n",
    "        for i in range(s[0]):\n",
    "            for j in range(s[1]):\n",
    "                corr_map1 = data1[i,j,:,:]\n",
    "                corr_map2 = data2[i,j,:,:]\n",
    "                correct_P1 = fdr_correction(corr_map1[:,1])\n",
    "                correct_P2 = fdr_correction(corr_map2[:,1])\n",
    "\n",
    "                tmp_data = np.zeros((s[2],2))\n",
    "                for k in range(s[2]):\n",
    "                    t1 = corr_map1[k,0]\n",
    "                    t2 = corr_map2[k,0]\n",
    "                    if t1*t2 > 0 and correct_P1[k] < 0.05 and correct_P2[k] < 0.05:\n",
    "                        odata1[i,j,k] = 1\n",
    "    elif t == 'bonferroni':\n",
    "        for i in range(s[0]):\n",
    "            for j in range(s[1]):\n",
    "                corr_map1 = data1[i,j,:,:]\n",
    "                corr_map2 = data2[i,j,:,:]\n",
    "                \n",
    "                tmp_data = np.zeros((s[2],2))\n",
    "                tp = 0.05/s[2]\n",
    "                for k in range(s[2]):\n",
    "                    t1 = corr_map1[k,0]\n",
    "                    t2 = corr_map2[k,0]\n",
    "                    if t1*t2 > 0 and corr_map1[k,1] < tp and corr_map2[k,1] < tp:\n",
    "                        odata1[i,j,k] = 1\n",
    "    else:\n",
    "        for i in range(s[0]):\n",
    "            for j in range(s[1]):\n",
    "                corr_map1 = data1[i,j,:,:]\n",
    "                corr_map2 = data2[i,j,:,:]\n",
    "                \n",
    "                tmp_data = np.zeros((s[2],2))\n",
    "                for k in range(s[2]):\n",
    "                    t1 = corr_map1[k,0]\n",
    "                    t2 = corr_map2[k,0]\n",
    "                    if t1*t2 > 0 and corr_map1[k,1] < float(t) and corr_map2[k,1] < float(t):\n",
    "                        odata1[i,j,k] = 1\n",
    "    \n",
    "    odata2 = np.zeros((s[0],s[2]))\n",
    "    for i in range(s[0]):\n",
    "        for j in range(s[2]):\n",
    "            tmp_data2 = odata1[i,:,j]\n",
    "            odata2[i,j] = np.sum(tmp_data2)/s[1]\n",
    "            \n",
    "    return odata2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = '/data/sliu/sampling_ukbb_analysis/new_results/'\n",
    "files = os.listdir(file_path)\n",
    "\n",
    "# t indictaes the significance thresholds: p<0.05, p<0.01, fdr_p<0.05, fdr_bonferroni<0.05\n",
    "t = '0.05'\n",
    "for f in files:\n",
    "    print(f)\n",
    "    CT_file_path1 = os.path.join(file_path,f) + '/random/random_data_CT1.npy'\n",
    "    CT_file_path2 = os.path.join(file_path,f) + '/random/random_data_CT2.npy'\n",
    "    CSA_file_path1 = os.path.join(file_path,f) + '/random/random_data_CSA1.npy'\n",
    "    CSA_file_path2 = os.path.join(file_path,f) + '/random/random_data_CSA2.npy'\n",
    "    FC_file_path1 = os.path.join(file_path,f) + '/random/random_data_FC1.npy'\n",
    "    FC_file_path2 = os.path.join(file_path,f) + '/random/random_data_FC2.npy'\n",
    "    \n",
    "    random_data_CSA1 = np.load(CSA_file_path1)\n",
    "    random_data_CT1 = np.load(CT_file_path1)\n",
    "    random_data_FC1= np.load(FC_file_path1)\n",
    "        \n",
    "    random_data_CSA2 = np.load(CSA_file_path2)\n",
    "    random_data_CT2 = np.load(CT_file_path2)\n",
    "    random_data_FC2 = np.load(FC_file_path2)\n",
    "    \n",
    "    CT_reliability = estimate_reliability(random_data_CT1,random_data_CT2,t=t)\n",
    "    CSA_reliability = estimate_reliability(random_data_CSA1,random_data_CSA2,t=t)\n",
    "    FC_reliability = estimate_reliability(random_data_FC1,random_data_FC2,t=t)\n",
    "    \n",
    "    CSA_file_name = 'new_results/'+f+'/CSA_reliability_'+t+'.csv'\n",
    "    CT_file_name = 'new_results/'+f+'/CT_reliability_'+t+'.csv'\n",
    "    FC_file_name = 'new_results/'+f+'/FC_reliability_'+t+'.csv'\n",
    "\n",
    "    data1 = pd.DataFrame(data=CT_reliability)\n",
    "    data2 = pd.DataFrame(data=CSA_reliability)\n",
    "    data3 = pd.DataFrame(data=FC_reliability)\n",
    "\n",
    "    data1.to_csv(CT_file_name,index=False)\n",
    "    data2.to_csv(CSA_file_name,index=False)\n",
    "    data3.to_csv(FC_file_name,index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
